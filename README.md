# Lokaal-LLM
Lokaal draaiende LLM-modellen met Flask en Ollama, inclusief een dynamische webinterface en model-switching.
